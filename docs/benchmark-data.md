# AI Agent Performance Benchmark Data

> **Source**: Performance benchmark chart - "Top Agents"
> **Date Collected**: 2025-07-05
> **Metric**: Overall performance score

## Top Performing Agents (Score: 15,000+)

| Rank | Agent | Model | Score | Performance Tier |
|------|-------|-------|-------|------------------|
| 1 | Claude Code | Sonnet 4 | 17,114.00 | Elite |
| 2 | OpenCode | Sonnet 4 | 16,874.00 | Elite |
| 3 | GitHub Copilot | Sonnet 4 | 16,774.00 | Elite |
| 4 | Trae | Sonnet 4 | 16,614.00 | Elite |
| 5 | Zed | Sonnet 4 | 16,314.00 | Elite |
| 6 | Claude Code | Opus | 16,254.00 | Elite |
| 7 | Warp dev | Sonnet 4 | 15,844.00 | Elite |

## High Performing Agents (Score: 13,000-15,000)

| Rank | Agent | Model | Score | Performance Tier |
|------|-------|-------|-------|------------------|
| 8 | Roo Code | Sonnet 4 | 15,714.00 | High |
| 9 | Cline | Sonnet 4 | 15,714.00 | High |
| 10 | Kilo Code | Sonnet 4 | 15,414.00 | High |
| 11 | Winsurf | Sonnet 4 | 14,824.00 | High |
| 12 | Cursor | Sonnet 4 | 14,814.00 | High |
| 13 | Augment Code | Sonnet 4 | 14,256.00 | High |
| 14 | Aider | Sonnet 4 | 13,354.00 | High |

## Medium Performing Agents (Score: <13,000)

| Rank | Agent | Model | Score | Performance Tier |
|------|-------|-------|-------|------------------|
| 15 | Void | Sonnet 4 | 9,200.00 | Medium |
| 16 | Gemini CLI | 2.5 Pro | 8,780.00 | Medium |
| 17 | Codex CLI | GPT 4.1 | 1,700.00 | Basic |

## Agent Categories by Function

### Code-Focused Development Agents
- **Claude Code** - Anthropic's comprehensive coding assistant
- **OpenCode** - Open-source code generation platform
- **GitHub Copilot** - Microsoft's AI pair programmer
- **Roo Code** - Specialized code completion agent
- **Cline** - Command-line interface coding agent
- **Kilo Code** - Lightweight code assistant
- **Augment Code** - Code enhancement and optimization agent
- **Aider** - AI-powered coding assistant
- **Codex CLI** - OpenAI's command-line code agent

### Development Environment Agents
- **Zed** - High-performance code editor agent
- **Warp dev** - Terminal and development workflow agent
- **Winsurf** - Web-based development environment agent
- **Cursor** - AI-first code editor agent
- **Void** - Minimalist development agent

### Specialized/Platform Agents
- **Trae** - Specialized agent (function TBD)
- **Gemini CLI** - Google's command-line AI agent

## Model Performance Analysis

### Sonnet 4 Dominance
- **Count**: 14 out of 17 agents use Sonnet 4
- **Average Score**: ~15,000 (estimated)
- **Top Performer**: Claude Code (17,114.00)
- **Score Range**: 9,200.00 - 17,114.00

### Alternative Models
- **Opus**: Claude Code (16,254.00) - Strong performance
- **Gemini 2.5 Pro**: Gemini CLI (8,780.00) - Moderate performance  
- **GPT 4.1**: Codex CLI (1,700.00) - Lowest performance

## Key Insights

### Performance Patterns
1. **Sonnet 4 models** consistently outperform other model variants
2. **Claude Code** leads in both Sonnet 4 and Opus configurations
3. **Development environment agents** (Zed, Warp, Cursor) show strong performance
4. **Traditional OpenAI models** (GPT 4.1) lag significantly behind

### Agent Specialization
- **Top tier** (15,000+): Comprehensive development tools
- **High tier** (13,000-15,000): Specialized coding assistants
- **Medium tier** (<13,000): Basic or experimental agents

## Research Questions

1. **Benchmark Methodology**: What specific tasks were measured?
2. **Scoring Criteria**: How are the numerical scores calculated?
3. **Test Environment**: What development scenarios were used?
4. **Model Versions**: Are these the latest versions of each model?
5. **Agent Configuration**: How were system prompts and tools configured?

## Future Analysis

### Priority Research Areas
- [ ] Document system prompts for top performers
- [ ] Analyze prompt engineering differences
- [ ] Compare tool integration strategies
- [ ] Study model-specific optimizations
- [ ] Investigate performance correlation factors

### Collection Priorities
1. **Elite Tier**: Claude Code, OpenCode, GitHub Copilot, Trae, Zed
2. **High Tier**: Cline, Cursor, Aider, Winsurf
3. **Research Interest**: Void, Gemini CLI (different model approaches)

---
*Benchmark data serves as foundation for systematic prompt collection and analysis*